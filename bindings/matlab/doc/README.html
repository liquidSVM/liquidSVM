<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <title></title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
  </style>
</head>
<body>
<h1 id="liquidsvm-for-matlab">liquidSVM for MATLAB</h1>
<p>Welcome to the MATLAB bindings for liquidSVM.</p>
<blockquote>
<p>This is a preview version of the new MATLAB bindings to liquidSVM, stay tuned for updates. On Windows there is a heavy Bug at the moment that renders it unusable.</p>
</blockquote>
<p>Both liquidSVM and these bindings are provided under the AGPL 3.0 license.</p>
<h2 id="installation">Installation</h2>
<ul>
<li>Download the Toolbox from <a href="http://www.isa.uni-stuttgart.de/software/matlab/liquidSVM.mltbx" class="uri">http://www.isa.uni-stuttgart.de/software/matlab/liquidSVM.mltbx</a> and install it in MATLAB by double clicking it.</li>
<li><p>You can compile the native library in MATLAB (for MacOS and Windows we currently ship binaries in the toolbox)</p>
<pre><code>mex -setup c++
makeliquidSVM native</code></pre>
<p>For this you need to have a compiler installed, and you might to issue <code>mex -setup c++</code> before.</p></li>
</ul>
<h2 id="usage">Usage</h2>
<div class="sourceCode"><pre class="sourceCode matlab"><code class="sourceCode matlab"><span class="co">% load some data sets with train/test split from http://www.isa.uni-stuttgart.de/liquidData/</span>
banana = liquidData(<span class="st">&#39;banana-bc&#39;</span>);  <span class="co">% binary labels</span>
banana_mc = liquidData(<span class="st">&#39;banana-mc&#39;</span>);  <span class="co">% labels with four unique values</span>
reg = liquidData(<span class="st">&#39;reg-1d&#39;</span>);  <span class="co">% real labels</span>

<span class="co">%% Least Squares Regression</span>
model = svm_ls(reg.train,<span class="st">&#39;DISPLAY&#39;</span>,<span class="st">&#39;1&#39;</span>);
[result, err] = model.test(reg.test);
result = model.predict(reg.testFeatures);

<span class="co">%% Mutli-Class classification</span>
model = svm_mc(banana_mc.train,<span class="st">&#39;DISPLAY&#39;</span>,<span class="st">&#39;1&#39;</span>,<span class="st">&#39;folds&#39;</span>,<span class="st">&#39;3&#39;</span>);
[result, err] = model.test(banana_mc.test);

<span class="co">%% Quantile Regression here for the 20%, 50%, and 80% quantiles</span>
model = svm_qt(reg.trainFeatures, reg.trainLabel,[<span class="fl">0.2</span>,<span class="fl">0.5</span>,<span class="fl">0.8</span>],<span class="st">&#39;DISPLAY&#39;</span>,<span class="st">&#39;1&#39;</span>);
[quantiles, err] = model.test(reg.testFeatures,reg.testLabel);
plot(reg.testFeatures, reg.testLabel, <span class="st">&#39;.&#39;</span>, reg.testFeatures, quantiles(:,<span class="fl">1</span>),<span class="st">&#39;.&#39;</span>,...
    reg.testFeatures, quantiles(:,<span class="fl">2</span>),<span class="st">&#39;.&#39;</span>,reg.testFeatures, quantiles(:,<span class="fl">3</span>),<span class="st">&#39;.&#39;</span>)

<span class="co">% now quantiles has three columns corresponding to the three requested quantiles</span>

<span class="co">%% Expectile Regression here for the 20% and 50% expectiles</span>
model = svm_ex(reg.trainFeatures, reg.trainLabel,[<span class="fl">.05</span>,<span class="fl">.5</span>],<span class="st">&#39;DISPLAY&#39;</span>,<span class="st">&#39;1&#39;</span>);
[expectiles, err] = model.test(reg.testFeatures,reg.testLabel);
plot(reg.testFeatures, reg.testLabel, <span class="st">&#39;.&#39;</span>, reg.testFeatures, expectiles(:,<span class="fl">1</span>),<span class="st">&#39;.&#39;</span>,...
    reg.testFeatures, expectiles(:,<span class="fl">2</span>),<span class="st">&#39;.&#39;</span>)

<span class="co">%% Receiver Operating Characteristic curve</span>
model = svm_roc(banana.trainFeatures, banana.trainLabel,<span class="fl">6</span>,<span class="st">&#39;DISPLAY&#39;</span>,<span class="st">&#39;1&#39;</span>);
[result, err] = model.test(banana.test);

<span class="co">%% Neyman-Pearson lemma</span>
model = svm_npl(banana.trainFeatures, banana.trainLabel, <span class="fl">1</span>,<span class="st">&#39;DISPLAY&#39;</span>,<span class="st">&#39;1&#39;</span>);
[result, err] = model.test(banana.test);

<span class="co">%% Write a solution (after train and select have been performed)</span>
model = svm_ls(reg.train,<span class="st">&#39;DISPLAY&#39;</span>,<span class="st">&#39;1&#39;</span>);
save myModelFile model
clear model

<span class="co">%% read a solution from file</span>
load myModelFile model
[result, err] = model.test(reg.test);</code></pre></div>
<p>The meaning of the configurations in the constructor is described in the next chapter.</p>
<blockquote>
<p><strong>NOTE:</strong> MATLAB does not respect flushing of print methods, hence setting <code>display</code> to <code>1</code> does not help in monitoring progress during execution because the output only shows at the end of the computation.</p>
</blockquote>
<blockquote>
<p><strong>NOTE:</strong> On macOS if you use MATLAB 2016a and Xcode 8 you have to make the new version available to MATLAB by changing <code>/Applications/MATLAB_R2015b.app/bin/maci64/mexopts/clang_maci64.xml</code> to also include <code>MacOSX10.12.sdk</code> on two occasions - similar details (for other versions) can be found int <a href="https://de.mathworks.com/matlabcentral/answers/243868-mex-can-t-find-compiler-after-xcode-7-update-r2015b" class="uri">https://de.mathworks.com/matlabcentral/answers/243868-mex-can-t-find-compiler-after-xcode-7-update-r2015b</a>. Remark that this change needs admin privileges.</p>
</blockquote>
<h3 id="octave">Octave</h3>
<p>Since Octave 4.0.x the <code>classdef</code> type of object-orientation is (experimentally) implemented so liquidSVM can be used there as well. Unzip the file <a href="http://www.isa.uni-stuttgart.de/software/matlab/liquidSVM-octave.zip" class="uri">http://www.isa.uni-stuttgart.de/software/matlab/liquidSVM-octave.zip</a> change into a directory, start octave and issue:</p>
<pre><code>makeliquidSVM native</code></pre>
<p>If this works you can use demo_svm etc. as above. ## Overview of Configuration Parameters</p>
<dl>
<dt><code>display</code></dt>
<dd>This parameter determines the amount of output of you see at the screen: The larger its value is, the more you see. This can help as a progress indication.
</dd>
<dt><code>scale</code></dt>
<dd><p>If set to a true value then for every feature in the training data a scaling is calculated so that its values lie in the interval <span class="math inline">[0, 1]</span>. The training then is performed using these scaled values and any testing data is scaled transparently as well.</p>
<p>Because SVMs are not scale-invariant any data should be scaled for two main reasons: First that all features have the same weight, and second to assure that the default gamma parameters that liquidSVM provide remain meaningful.</p>
<p>If you do not have scaled the data previously this is an easy option.</p>
</dd>
<dt><code>threads</code></dt>
<dd><p>This parameter determines the number of cores used for computing the kernel matrices, the validation error, and the test error.</p>
<ul>
<li><code>threads=0</code> (default) means that all physical cores of your CPU run one thread.</li>
<li><code>threads=-1</code> means that all but one physical cores of your CPU run one thread.</li>
</ul>
</dd>
<dt><code>partition_choice</code></dt>
<dd><p>This parameter determines the way the input space is partitioned. This allows larger data sets for which the kernel matrix does not fit into memory.</p>
<ul>
<li><code>partition_choice=0</code> (default) disables partitioning.</li>
<li><code>partition_choice=6</code> gives usually highest speed.</li>
<li><code>partition_choice=5</code> gives usually the best test error.</li>
</ul>
</dd>
<dt><code>grid_choice</code></dt>
<dd>This parameter determines the size of the hyper- parameter grid used during the training phase. Larger values correspond to larger grids. By default, a 10x10 grid is used. Exact descriptions are given in the next section.
</dd>
<dt><code>adaptivity_control</code></dt>
<dd>This parameter determines, whether an adaptive grid search heuristic is employed. Larger values lead to more aggressive strategies. The default <code>adaptivity_control = 0</code> disables the heuristic.
</dd>
<dt><code>random_seed</code></dt>
<dd>This parameter determines the seed for the random generator. <code>random_seed</code> = -1 uses the internal timer create the seed. All other values lead to repeatable behavior of the svm.
</dd>
<dt><code>folds</code></dt>
<dd>How many folds should be used.
</dd>
</dl>
<h2 id="specialized-configuration-parameters">Specialized configuration parameters</h2>
<p>Parameters for regression (least-squares, quantile, and expectile)</p>
<dl>
<dt><code>clipping</code></dt>
<dd>This parameter determines whether the decision functions should be clipped at the specified value. The value <code>clipping</code> = -1.0 leads to an adaptive clipping value, whereas <code>clipping</code> = 0 disables clipping.
</dd>
</dl>
<p>Parameter for multiclass classification determine the multiclass strategy: <code>mc-type=0</code> : AvA with hinge loss. <code>mc-type=1</code> : OvA with least squares loss. <code>mc-type=2</code> : OvA with hinge loss. <code>mc-type=3</code> : AvA with least squares loss.</p>
<p>Parameters for Neyman-Pearson Learning</p>
<dl>
<dt><code>class</code></dt>
<dd>The class, the <code>constraint</code> is enforced on.
</dd>
<dt><code>constraint</code></dt>
<dd>The constraint on the false alarm rate. The script actually considers a couple of values around the value of <code>constraint</code> to give the user an informed choice.
</dd>
</dl>
<h2 id="hyperparameter-grid">Hyperparameter Grid</h2>
<p>For Support Vector Machines two hyperparameters need to be determined:</p>
<ul>
<li><code>gamma</code> the bandwith of the kernel</li>
<li><code>lambda</code> has to be chosen such that neither over- nor underfitting happen. lambda values are the classical regularization parameter in front of the norm term.</li>
</ul>
<p>liquidSVM has a built-in a cross-validation scheme to calculate validation errors for many values of these hyperparameters and then to choose the best pair. Since there are two parameters this means we consider a two-dimensional grid.</p>
<p>For both parameters either specific values can be given or a geometrically spaced grid can be specified.</p>
<dl>
<dt><code>gamma_steps</code>, <code>min_gamma</code>, <code>max_gamma</code></dt>
<dd>specifies in the interval between <code>min_gamma</code> and <code>max_gamma</code> there should be <code>gamma_steps</code> many values
</dd>
<dt><code>gammas</code></dt>
<dd>e.g. <code>gammas=c(0.1,1,10,100)</code> will do these four gamma values
</dd>
<dt><code>lambda_steps</code>, <code>min_lambda</code>, <code>max_lambda</code></dt>
<dd>specifies in the interval between <code>min_lambda</code> and <code>max_lambda</code> there should be <code>lambda_steps</code> many values
</dd>
<dt><code>lambdas</code></dt>
<dd>e.g. <code>lambdas=c(0.1,1,10,100)</code> will do these four lambda values
</dd>
<dt><code>c_values</code></dt>
<dd>the classical term in front of the empirical error term, e.g. <code>c_values=c(0.1,1,10,100)</code> will do these four cost values (basically inverse of <code>lambdas</code>)
</dd>
</dl>
<p>Note the min and max values are scaled according the the number of samples, the dimensionality of the data sets, the number of folds used, and the estimated diameter of the data set.</p>
<p>Using <code>grid_choice</code> allows for some general choices of these parameters</p>
<table>
<thead>
<tr class="header">
<th><code>grid_choice</code></th>
<th>0</th>
<th>1</th>
<th>2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>gamma_steps</code></td>
<td>10</td>
<td>15</td>
<td>20</td>
</tr>
<tr class="even">
<td><code>lambda_steps</code></td>
<td>10</td>
<td>15</td>
<td>20</td>
</tr>
<tr class="odd">
<td><code>min_gamma</code></td>
<td>0.2</td>
<td>0.1</td>
<td>0.05</td>
</tr>
<tr class="even">
<td><code>max_gamma</code></td>
<td>5.0</td>
<td>10.0</td>
<td>20.0</td>
</tr>
<tr class="odd">
<td><code>min_lambda</code></td>
<td>0.001</td>
<td>0.0001</td>
<td>0.00001</td>
</tr>
<tr class="even">
<td><code>max_lambda</code></td>
<td>0.01</td>
<td>0.01</td>
<td>0.01</td>
</tr>
</tbody>
</table>
<p>Using negative values of <code>grid_choice</code> we create a grid with listed gamma and lambda values:</p>
<table>
<thead>
<tr class="header">
<th><code>grid_choice</code></th>
<th>-1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>gammas</code></td>
<td><code>c(10.0, 5.0, 2.0, 1.0, 0.5, 0.25, 0.1, 0.05)</code></td>
</tr>
<tr class="even">
<td><code>lambdas</code></td>
<td><code>c(1.0, 0.1, 0.01, 0.001, 0.0001, 0.00001, 0.000001, 0.0000001)</code></td>
</tr>
</tbody>
</table>
<table>
<thead>
<tr class="header">
<th><code>grid_choice</code></th>
<th>-2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>gammas</code></td>
<td><code>c(10.0, 5.0, 2.0, 1.0, 0.5, 0.25, 0.1, 0.05)</code></td>
</tr>
<tr class="even">
<td><code>c_values</code></td>
<td><code>c(0.01, 0.1, 1, 10, 100, 1000, 10000)</code></td>
</tr>
</tbody>
</table>
<h2 id="adaptive-grid">Adaptive Grid</h2>
<p>An adaptive grid search can be activated. The higher the values of <code>MAX_LAMBDA_INCREASES</code> and <code>MAX_NUMBER_OF_WORSE_GAMMAS</code> are set the more conservative the search strategy is. The values can be freely modified.</p>
<table>
<thead>
<tr class="header">
<th><code>ADAPTIVITY_CONTROL</code></th>
<th>1</th>
<th>2</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>MAX_LAMBDA_INCREASES</code></td>
<td>4</td>
<td>3</td>
</tr>
<tr class="even">
<td><code>MAX_NUMBER_OF_WORSE_GAMMAS</code></td>
<td>4</td>
<td>3</td>
</tr>
</tbody>
</table>
<h2 id="cells">Cells</h2>
<p>A major issue with SVMs is that for larger sample sizes the kernel matrix does not fit into the memory any more. Classically this gives an upper limit for the class of problems that traditional SVMs can handle without significant runtime increase. Furthermore also the time complexity is at least <span class="math inline"><em>O</em>(<em>n</em><sup>2</sup>)</span>.</p>
<p>liquidSVM implements two major concepts to circumvent these issues. One is random chunks which is known well in the literature. However we prefer the new alternative of splitting the space into spatial cells and use local SVMs on every cell.</p>
<p>If you specify <code>useCells=TRUE</code> then the sample space <span class="math inline"><em>X</em></span> gets partitioned into a number of cells. The training is done first for cell 1 then for cell 2 and so on. Now, to predict the label for a value <span class="math inline"><em>x</em> ∈ <em>X</em></span> liquidSVM first finds out to which cell this <span class="math inline"><em>x</em></span> belongs and then uses the SVM of that cell to predict a label for it.</p>
<blockquote>
<p>If you run into memory issues turn cells on: <code>useCells=TRUE</code></p>
</blockquote>
<p>This is quite performant, since the complexity in both time and memore are both <span class="math inline"><em>O</em>(CELLSIZE × <em>n</em>)</span> and this holds both for training as well as testing! It also can be shown that the quality of the solution is comparable, at least for moderate dimensions.</p>
<p>The cells can be configured using the <code>partition_choice</code>:</p>
<ol style="list-style-type: decimal">
<li><p>This gives a partition into random chunks of size 2000</p>
<p><code>VORONOI=c(1, 2000)</code></p></li>
<li><p>This gives a partition into 10 random chunks</p>
<p><code>VORONOI=c(2, 10)</code></p></li>
<li><p>This gives a Voronoi partition into cells with radius not larger than 1.0. For its creation a subsample containing at most 50.000 samples is used.</p>
<p><code>VORONOI=c(3, 1.0, 50000)</code></p></li>
<li><p>This gives a Voronoi partition into cells with at most 2000 samples (approximately). For its creation a subsample containing at most 50.000 samples is used. A shrinking heuristic is used to reduce the number of cells.</p>
<p><code>VORONOI=c(4, 2000, 1, 50000)</code></p></li>
<li><p>This gives a overlapping regions with at most 2000 samples (approximately). For its creation a subsample containing at most 50.000 samples is used. A stopping heuristic is used to stop the creation of regions if 0.5 * 2000 samples have not been assigned to a region, yet.</p>
<p><code>VORONOI=c(5, 2000, 0.5, 50000, 1)</code></p></li>
<li><p>This splits the working sets into Voronoi like with <code>PARTITION_TYPE=4</code>. Unlike that case, the centers for the Voronoi partition are found by a recursive tree approach, which in many cases may be faster.</p>
<p><code>VORONOI=c(6, 2000, 1, 50000, 2.0, 20, 4,)</code></p></li>
</ol>
<p>The first parameter values correspond to <code>NO_PARTITION</code>, <code>RANDOM_CHUNK_BY_SIZE</code>, <code>RANDOM_CHUNK_BY_NUMBER</code>, <code>VORONOI_BY_RADIUS</code>, <code>VORONOI_BY_SIZE</code>, <code>OVERLAP_BY_SIZE</code></p>
<h2 id="weights">Weights</h2>
<ul>
<li><p>qt, ex: Here the number of considered tau-quantiles/expectiles as well as the considered tau-values are defined. You can freely change these values but notice that the list of tau-values is space-separated!</p></li>
<li><p>npl, roc: Here, you define, which weighted classification problems will be considered. The choice is usually a bit tricky. Good luck ...</p></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">NPL:
WEIGHT_STEPS=<span class="dv">10</span>
MIN_WEIGHT=<span class="fl">0.001</span>
MAX_WEIGHT=<span class="fl">0.5</span>
GEO_WEIGHTS=<span class="dv">1</span>

ROC:
WEIGHT_STEPS=<span class="dv">9</span>
MAX_WEIGHT=<span class="fl">0.9</span>
MIN_WEIGHT=<span class="fl">0.1</span>
GEO_WEIGHTS=<span class="dv">0</span></code></pre></div>
<h2 id="more-advanced-parameters">More Advanced Parameters</h2>
<p>The following parameters should only employed by experienced users and are self-explanatory for these:</p>
<dl>
<dt><code>KERNEL</code></dt>
<dd>specifies the kernel to use, at the moment either <code>GAUSS_RBF</code> or <code>POISSON</code>
</dd>
<dt><code>RETRAIN_METHOD</code></dt>
<dd>After training on grids and folds there are only solutions on folds. In order to construct a global solution one can either retrain on the whole training data (<code>SELECT_ON_ENTIRE_TRAIN_SET</code>) or the (partial) solutions from the training are kept and combined using voting (<code>SELECT_ON_EACH_FOLD</code> default)
</dd>
<dt><code>store_solutions_internally</code></dt>
<dd>If this is true (default in all applicable cases) then the solutions of the train phase are stored and can be just reused in the select phase. If you slowly run out of memory during the train phase maybe disable this. However then in the select phase the best models have to be trained again.
</dd>
</dl>
<p>For completeness here are some values that usually get set by the learning scenario</p>
<dl>
<dt><code>SVM_TYPE</code></dt>
<dd><code>KERNEL_RULE</code>, <code>SVM_LS_2D</code>, <code>SVM_HINGE_2D</code>, <code>SVM_QUANTILE</code>, <code>SVM_EXPECTILE_2D</code>, <code>SVM_TEMPLATE</code>
</dd>
<dt><code>LOSS_TYPE</code></dt>
<dd><code>CLASSIFICATION_LOSS</code>, <code>MULTI_CLASS_LOSS</code>, <code>LEAST_SQUARES_LOSS</code>, <code>WEIGHTED_LEAST_SQUARES_LOSS</code>, <code>PINBALL_LOSS</code>, <code>TEMPLATE_LOSS</code>
</dd>
<dt><code>VOTE_SCENARIO</code></dt>
<dd><code>VOTE_CLASSIFICATION</code>, <code>VOTE_REGRESSION</code>, <code>VOTE_NPL</code>
</dd>
<dt><code>KERNEL_MEMORY_MODEL</code></dt>
<dd><code>LINE_BY_LINE</code>, <code>BLOCK</code>, <code>CACHE</code>, <code>EMPTY</code>
</dd>
<dt><code>FOLDS_KIND</code></dt>
<dd><code>FROM_FILE</code>, <code>BLOCKS</code>, <code>ALTERNATING</code>, <code>RANDOM</code>, <code>STRATIFIED</code>, <code>RANDOM_SUBSET</code>
</dd>
<dt><code>WS_TYPE</code></dt>
<dd><code>FULL_SET</code>, <code>MULTI_CLASS_ALL_VS_ALL</code>, <code>MULTI_CLASS_ONE_VS_ALL</code>, <code>BOOT_STRAP</code>
</dd>
</dl>
</body>
</html>
